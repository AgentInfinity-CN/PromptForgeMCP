#!/usr/bin/env python3
"""
æ‰§è¡Œå·¥å…·æ¨¡å—
æä¾›æç¤ºæ‰§è¡Œç›¸å…³çš„MCPå·¥å…·
"""

import time
from typing import Dict, Optional
from fastmcp import Context
from pydantic import Field
from promptforge_mcp.models.models import ExecutionResult
from promptforge_mcp.config.config import Config
from promptforge_mcp.services.ai_service import AIServiceManager


async def execute_prompt(
    prompt: str = Field(description="è¦æ‰§è¡Œçš„æç¤º"),
    model: str = Field(default="", description="AIæ¨¡å‹åç§°ï¼Œç©ºå€¼æ—¶ä½¿ç”¨é…ç½®é»˜è®¤"),
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="åˆ›é€ æ€§æ¸©åº¦å‚æ•°"),
    max_tokens: int = Field(default=1000, ge=1, le=4000, description="æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°"),
    variables: Optional[Dict[str, str]] = Field(default=None, description="æç¤ºå˜é‡æ›¿æ¢"),
    config: Config = None,
    ai_service: AIServiceManager = None,
    ctx: Context = None
) -> ExecutionResult:
    """æ‰§è¡Œå•ä¸ªæ¨¡å‹çš„æç¤º"""
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
    if not model:
        model = config.default_execution_model
    
    # éªŒè¯æç¤ºé•¿åº¦
    if len(prompt) > config.max_prompt_length:
        await ctx.error(f"âŒ æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)")
        return ExecutionResult(
            success=False,
            response="",
            model=model,
            execution_time=0,
            token_usage={},
            error_message=f"æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)"
        )
    
    await ctx.info(f"âš¡ æ‰§è¡Œæç¤ºï¼Œæ¨¡å‹: {model}")
    start_time = time.time()
    
    try:
        # å˜é‡æ›¿æ¢
        if variables:
            for key, value in variables.items():
                prompt = prompt.replace(f"{{{key}}}", value)
            await ctx.info(f"ğŸ”„ å·²æ›¿æ¢ {len(variables)} ä¸ªå˜é‡")
        
        await ctx.report_progress(25, 100, "å‡†å¤‡æ‰§è¡Œ...")
        
        # æ‰§è¡ŒAIè°ƒç”¨
        messages = [{"role": "user", "content": prompt}]
        response = await ai_service.call_ai(messages, model, temperature, max_tokens)
        
        execution_time = time.time() - start_time
        await ctx.report_progress(100, 100, "æ‰§è¡Œå®Œæˆ")
        await ctx.info(f"âœ… æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        return ExecutionResult(
            success=True,
            response=response,
            model=model,
            execution_time=execution_time,
            token_usage={"input": len(prompt.split()), "output": len(response.split())},
        )
        
    except Exception as e:
        execution_time = time.time() - start_time
        await ctx.error(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        return ExecutionResult(
            success=False,
            response="",
            model=model,
            execution_time=execution_time,
            token_usage={},
            error_message=str(e)
        ) 