#!/usr/bin/env python3
"""
åˆ†æå·¥å…·æ¨¡å—
æä¾›æç¤ºåˆ†æç›¸å…³çš„MCPå·¥å…·
"""

import time
from typing import Literal
from fastmcp import Context
from pydantic import Field
from promptforge_mcp.models.models import AnalysisResult
from promptforge_mcp.config.config import Config
from promptforge_mcp.services.ai_service import AIServiceManager


async def analyze_prompt(
    prompt: str = Field(description="è¦åˆ†æçš„æç¤ºæ–‡æœ¬"),
    model: str = Field(default="", description="ä½¿ç”¨çš„AIæ¨¡å‹ï¼Œç©ºå€¼æ—¶ä½¿ç”¨é…ç½®é»˜è®¤"),
    analysis_type: Literal["quick", "detailed", "dual"] = Field(default="dual", description="åˆ†æç±»å‹"),
    config: Config = None,
    ai_service: AIServiceManager = None,
    ctx: Context = None
) -> AnalysisResult:
    """æ‰§è¡Œæç¤ºåˆ†æï¼Œè¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœ"""
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
    if not model:
        model = config.default_analysis_model
    
    # éªŒè¯æç¤ºé•¿åº¦
    if len(prompt) > config.max_prompt_length:
        await ctx.error(f"âŒ æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)")
        return AnalysisResult(
            success=False,
            quick_report=None,
            detailed_report=None,
            metrics=ai_service.calculate_metrics(prompt),
            suggestions=[],
            error_message=f"æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)"
        )
    
    await ctx.info(f"ğŸ” å¼€å§‹åˆ†ææç¤ºï¼Œæ¨¡å¼: {analysis_type}, æ¨¡å‹: {model}")
    
    try:
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        metrics = ai_service.calculate_metrics(prompt)
        await ctx.report_progress(25, 100, "è®¡ç®—æŒ‡æ ‡å®Œæˆ")
        
        quick_report = None
        detailed_report = None
        
        if analysis_type in ["quick", "dual"]:
            quick_messages = [
                {"role": "system", "content": "ä½ æ˜¯æç¤ºå·¥ç¨‹ä¸“å®¶ï¼Œè¯·å¯¹æç¤ºè¿›è¡Œå¿«é€Ÿåˆ†æã€‚"},
                {"role": "user", "content": f"åˆ†æè¿™ä¸ªæç¤ºï¼š\n{prompt}"}
            ]
            quick_report = await ai_service.call_ai(quick_messages, model, 0.3, 500)
            await ctx.report_progress(60, 100, "å¿«é€Ÿåˆ†æå®Œæˆ")
        
        if analysis_type in ["detailed", "dual"]:
            detailed_messages = [
                {"role": "system", "content": "ä½ æ˜¯é«˜çº§æç¤ºå·¥ç¨‹å¸ˆï¼Œè¯·è¿›è¡Œè¯¦ç»†çš„æç¤ºåˆ†æã€‚"},
                {"role": "user", "content": f"è¯¦ç»†åˆ†æè¿™ä¸ªæç¤ºï¼š\n{prompt}"}
            ]
            detailed_report = await ai_service.call_ai(detailed_messages, model, 0.5, 1500)
            await ctx.report_progress(90, 100, "è¯¦ç»†åˆ†æå®Œæˆ")
        
        # ç”ŸæˆAIé©±åŠ¨çš„ä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®®
        await ctx.report_progress(95, 100, "ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        # å°†åˆ†ææŠ¥å‘Šä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™å»ºè®®ç”Ÿæˆå™¨
        analysis_context = ""
        if quick_report:
            analysis_context += f"å¿«é€Ÿåˆ†æï¼š{quick_report[:200]}..."
        if detailed_report:
            analysis_context += f"\nè¯¦ç»†åˆ†æï¼š{detailed_report[:300]}..."
        
        suggestions = await ai_service.generate_suggestions(
            prompt=prompt,
            model=model,
            analysis_context=analysis_context
        )
        
        await ctx.report_progress(100, 100, "åˆ†æå®Œæˆ")
        await ctx.info("âœ… æç¤ºåˆ†ææˆåŠŸå®Œæˆ")
        
        return AnalysisResult(
            success=True,
            quick_report=quick_report,
            detailed_report=detailed_report,
            metrics=metrics,
            suggestions=suggestions
        )
        
    except Exception as e:
        await ctx.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        return AnalysisResult(
            success=False,
            quick_report=None,
            detailed_report=None,
            metrics=ai_service.calculate_metrics(prompt),
            suggestions=[],
            error_message=str(e)
        ) 