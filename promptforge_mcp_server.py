#!/usr/bin/env python3
"""
PromptForge MCP Server
====================

åŸºäºFastMCPæ¡†æ¶çš„PromptForge AIæç¤ºå·¥ç¨‹æœåŠ¡
æä¾›æç¤ºåˆ†æã€æ‰§è¡Œã€è¯„ä¼°ç”Ÿæˆå’Œåº“ç®¡ç†åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•:
    python promptforge_mcp_server.py                    # STDIOæ¨¡å¼ (Claude Desktop)
    python promptforge_mcp_server.py --http             # HTTPæ¨¡å¼
    python promptforge_mcp_server.py --http --port 8080 # æŒ‡å®šç«¯å£
"""

import asyncio
import json
import logging
import sqlite3
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Literal, Optional, Any, Union
import argparse
import os
import httpx

from fastmcp import FastMCP, Context
from pydantic import BaseModel, Field
from dotenv import load_dotenv

# ================== æ•°æ®æ¨¡å‹å®šä¹‰ ==================

class PromptMetrics(BaseModel):
    """æç¤ºæŒ‡æ ‡ç»Ÿè®¡"""
    characters: int = Field(description="å­—ç¬¦æ•°")
    words: int = Field(description="è¯æ•°")
    lines: int = Field(description="è¡Œæ•°")
    special_chars: List[str] = Field(description="ç‰¹æ®Šå­—ç¬¦")

class AnalysisResult(BaseModel):
    """åˆ†æç»“æœæ¨¡å‹"""
    success: bool = Field(description="åˆ†ææ˜¯å¦æˆåŠŸ")
    quick_report: Optional[str] = Field(description="å¿«é€Ÿåˆ†ææŠ¥å‘Š")
    detailed_report: Optional[str] = Field(description="è¯¦ç»†åˆ†ææŠ¥å‘Š")
    metrics: PromptMetrics = Field(description="æç¤ºæŒ‡æ ‡ç»Ÿè®¡")
    suggestions: List[str] = Field(description="ä¼˜åŒ–å»ºè®®")
    error_message: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")

class ExecutionResult(BaseModel):
    """æ‰§è¡Œç»“æœæ¨¡å‹"""
    success: bool = Field(description="æ‰§è¡Œæ˜¯å¦æˆåŠŸ")
    response: str = Field(description="AIæ¨¡å‹å“åº”")
    model: str = Field(description="ä½¿ç”¨çš„æ¨¡å‹")
    execution_time: float = Field(description="æ‰§è¡Œæ—¶é—´(ç§’)")
    token_usage: Dict[str, int] = Field(description="ä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡")
    error_message: Optional[str] = Field(default=None, description="é”™è¯¯ä¿¡æ¯")



class SavedPrompt(BaseModel):
    """ä¿å­˜çš„æç¤º"""
    id: int = Field(description="æç¤ºID")
    title: str = Field(description="æç¤ºæ ‡é¢˜")
    content: str = Field(description="æç¤ºå†…å®¹")
    description: str = Field(description="æè¿°ä¿¡æ¯")
    category: str = Field(description="åˆ†ç±»")
    tags: List[str] = Field(description="æ ‡ç­¾åˆ—è¡¨")
    created_at: str = Field(description="åˆ›å»ºæ—¶é—´")
    updated_at: str = Field(description="æ›´æ–°æ—¶é—´")
    usage_count: int = Field(description="ä½¿ç”¨æ¬¡æ•°")

# ================== é…ç½®å’Œå·¥å…·ç±» ==================

class Config:
    """é…ç½®ç®¡ç† - ä» .env æ–‡ä»¶åŠ è½½æ‰€æœ‰é…ç½®å‚æ•°"""
    def __init__(self, env_file: str = ".env"):
        # åŠ è½½ .env æ–‡ä»¶
        load_dotenv(env_file, override=True)
        
        # AI æœåŠ¡æä¾›å•†é…ç½®
        self.openai_api_key = os.getenv("OPENAI_API_KEY", "")
        self.openai_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY", "")
        self.anthropic_base_url = os.getenv("ANTHROPIC_BASE_URL", "https://api.anthropic.com")
        self.default_provider = os.getenv("DEFAULT_AI_PROVIDER", "anthropic")
        
        # æ•°æ®åº“é…ç½®
        self.database_path = os.getenv("DATABASE_PATH", "promptforge.db")
        
        # æœåŠ¡é…ç½®
        self.mcp_server_port = int(os.getenv("MCP_SERVER_PORT", "8080"))
        self.log_level = os.getenv("LOG_LEVEL", "INFO")
        self.max_prompt_length = int(os.getenv("MAX_PROMPT_LENGTH", "50000"))
        self.analysis_timeout = int(os.getenv("ANALYSIS_TIMEOUT", "30"))
        self.execution_timeout = int(os.getenv("EXECUTION_TIMEOUT", "60"))
        
        # AI æ¨¡å‹é…ç½®
        self.default_analysis_model = os.getenv("DEFAULT_ANALYSIS_MODEL", "claude-3-sonnet-20240229")
        self.default_execution_model = os.getenv("DEFAULT_EXECUTION_MODEL", "claude-3-sonnet-20240229")
        self.openai_model = os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview")
        
        # å®‰å…¨é…ç½®
        self.allowed_hosts = os.getenv("ALLOWED_HOSTS", "localhost,127.0.0.1").split(",")
        self.debug_mode = os.getenv("DEBUG_MODE", "false").lower() == "true"
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        logging.basicConfig(level=getattr(logging, self.log_level.upper()))
        
    def get_available_providers(self) -> Dict[str, bool]:
        """è·å–å¯ç”¨çš„AIæä¾›å•†"""
        return {
            "openai": bool(self.openai_api_key),
            "anthropic": bool(self.anthropic_api_key)
        }
    
    def get_model_for_provider(self, provider: str) -> str:
        """æ ¹æ®æä¾›å•†è·å–é»˜è®¤æ¨¡å‹"""
        model_map = {
            "openai": self.openai_model,
            "anthropic": self.default_analysis_model
        }
        return model_map.get(provider, self.default_analysis_model)

class DatabaseManager:
    """æ•°æ®åº“ç®¡ç†å™¨"""
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.init_database()
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæç¤ºåº“è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS saved_prompts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                description TEXT DEFAULT '',
                category TEXT DEFAULT 'General',
                tags TEXT DEFAULT '[]',
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                usage_count INTEGER DEFAULT 0
            )
        """)
        
        # åˆ›å»ºæ‰§è¡Œå†å²è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS execution_history (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                prompt TEXT NOT NULL,
                model TEXT NOT NULL,
                temperature REAL NOT NULL,
                max_tokens INTEGER,
                success BOOLEAN NOT NULL,
                response TEXT,
                error_msg TEXT,
                execution_time REAL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        
        conn.commit()
        conn.close()
    
    def save_prompt(self, title: str, content: str, description: str = "", 
                   category: str = "General", tags: List[str] = None) -> SavedPrompt:
        """ä¿å­˜æç¤º"""
        if tags is None:
            tags = []
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT INTO saved_prompts (title, content, description, category, tags)
            VALUES (?, ?, ?, ?, ?)
        """, (title, content, description, category, json.dumps(tags)))
        
        prompt_id = cursor.lastrowid
        conn.commit()
        conn.close()
        
        return self.get_prompt(prompt_id)
    
    def get_prompt(self, prompt_id: int) -> Optional[SavedPrompt]:
        """è·å–æç¤º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT * FROM saved_prompts WHERE id = ?", (prompt_id,))
        row = cursor.fetchone()
        conn.close()
        
        if row:
            return SavedPrompt(
                id=row[0], title=row[1], content=row[2], description=row[3],
                category=row[4], tags=json.loads(row[5]), created_at=row[6],
                updated_at=row[7], usage_count=row[8]
            )
        return None
    
    def search_prompts(self, query: str = "", category: str = "", 
                      tags: List[str] = None, limit: int = 20) -> List[SavedPrompt]:
        """æœç´¢æç¤º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        where_conditions = []
        params = []
        
        if query:
            where_conditions.append("(title LIKE ? OR content LIKE ? OR description LIKE ?)")
            query_param = f"%{query}%"
            params.extend([query_param, query_param, query_param])
        
        if category:
            where_conditions.append("category = ?")
            params.append(category)
        
        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        
        cursor.execute(f"""
            SELECT * FROM saved_prompts 
            WHERE {where_clause}
            ORDER BY updated_at DESC 
            LIMIT ?
        """, params + [limit])
        
        rows = cursor.fetchall()
        conn.close()
        
        results = []
        for row in rows:
            # æ ‡ç­¾è¿‡æ»¤
            prompt_tags = json.loads(row[5])
            if tags and not any(tag in prompt_tags for tag in tags):
                continue
                
            results.append(SavedPrompt(
                id=row[0], title=row[1], content=row[2], description=row[3],
                category=row[4], tags=prompt_tags, created_at=row[6],
                updated_at=row[7], usage_count=row[8]
            ))
        
        return results
    
    def delete_prompt(self, prompt_id: int) -> bool:
        """åˆ é™¤æç¤º"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("DELETE FROM saved_prompts WHERE id = ?", (prompt_id,))
        deleted = cursor.rowcount > 0
        
        conn.commit()
        conn.close()
        
        return deleted

class AIServiceManager:
    """AIæœåŠ¡ç®¡ç†å™¨ - æ”¯æŒOpenAIå’ŒAnthropic"""
    def __init__(self, config: Config):
        self.config = config
        self.client = httpx.AsyncClient(timeout=60.0)
    
    async def call_ai(self, messages: List[Dict], model: str = "", 
                     temperature: float = 0.7, max_tokens: int = 1000) -> str:
        """è°ƒç”¨AIæœåŠ¡ - æ ¹æ®æ¨¡å‹è‡ªåŠ¨é€‰æ‹©æä¾›å•†"""
        
        # éªŒè¯è¾“å…¥å‚æ•°
        self._validate_messages(messages)
        
        if temperature < 0 or temperature > 2:
            raise ValueError("temperatureå‚æ•°å¿…é¡»åœ¨0-2ä¹‹é—´")
        
        if max_tokens < 0:
            raise ValueError("max_tokenså‚æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
        if not model:
            model = self.config.default_execution_model
        
        # æ ¹æ®æ¨¡å‹åç§°åˆ¤æ–­ä½¿ç”¨å“ªä¸ªæä¾›å•†
        provider = self._get_provider_from_model(model)
        
        if provider == "anthropic":
            return await self._call_anthropic(messages, model, temperature, max_tokens)
        elif provider == "openai":
            return await self._call_openai(messages, model, temperature, max_tokens)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def _call_openai(self, messages: List[Dict], model: str, temperature: float, max_tokens: int) -> str:
        """è°ƒç”¨OpenAI API"""
        if not self.config.openai_api_key:
            raise ValueError("OpenAI APIå¯†é’¥æœªé…ç½®")
        
        # é»˜è®¤æ¨¡å‹
        if not model:
            model = "gpt-4"
        
        request_data = {
            "model": model,
            "messages": messages,
            "temperature": temperature
        }
        
        if max_tokens > 0:
            request_data["max_tokens"] = max_tokens
        
        endpoint = f"{self.config.openai_base_url.rstrip('/')}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.config.openai_api_key}"
        }
        
        try:
            response = await self.client.post(endpoint, json=request_data, headers=headers)
            response.raise_for_status()
            
            result = response.json()
            if not result.get("choices"):
                raise ValueError("OpenAI APIè¿”å›ç©ºå“åº”")
            
            return result["choices"][0]["message"]["content"]
            
        except httpx.HTTPStatusError as e:
            error_text = e.response.text if hasattr(e.response, 'text') else str(e)
            raise ValueError(f"OpenAI APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {e.response.status_code}): {error_text}")
        except Exception as e:
            raise ValueError(f"OpenAI APIè°ƒç”¨é”™è¯¯: {str(e)}")
    
    async def _call_anthropic(self, messages: List[Dict], model: str, temperature: float, max_tokens: int) -> str:
        """è°ƒç”¨Anthropic API"""
        if not self.config.anthropic_api_key:
            raise ValueError("Anthropic APIå¯†é’¥æœªé…ç½®")
        
        # é»˜è®¤æ¨¡å‹
        if not model:
            model = "claude-3-7-sonnet-20250219"
        
        # Anthropic temperatureèŒƒå›´æ˜¯0-1ï¼ŒOpenAIæ˜¯0-2ï¼Œéœ€è¦è½¬æ¢
        anthropic_temperature = min(max(temperature / 2.0 if temperature > 1.0 else temperature, 0), 1)
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼ - Anthropicéœ€è¦åˆ†ç¦»systemæ¶ˆæ¯
        anthropic_messages = []
        system_message = ""
        
        for msg in messages:
            if msg.get("role") == "system":
                system_message = msg.get("content", "")
            else:
                anthropic_messages.append({
                    "role": msg.get("role"),
                    "content": msg.get("content", "")
                })
        
        request_data = {
            "model": model,
            "max_tokens": max_tokens if max_tokens > 0 else 1000,  # Anthropicè¦æ±‚max_tokens
            "temperature": anthropic_temperature,
            "messages": anthropic_messages
        }
        
        if system_message:
            request_data["system"] = system_message
        
        endpoint = f"{self.config.anthropic_base_url.rstrip('/')}/v1/messages"
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.anthropic_api_key,
            "anthropic-version": "2023-06-01"
        }
        
        try:
            response = await self.client.post(endpoint, json=request_data, headers=headers)
            response.raise_for_status()
            
            result = response.json()
            if not result.get("content"):
                raise ValueError("Anthropic APIè¿”å›ç©ºå“åº”")
            
            return result["content"][0]["text"]
            
        except httpx.HTTPStatusError as e:
            error_text = e.response.text if hasattr(e.response, 'text') else str(e)
            raise ValueError(f"Anthropic APIè¯·æ±‚å¤±è´¥ (çŠ¶æ€ç : {e.response.status_code}): {error_text}")
        except Exception as e:
            raise ValueError(f"Anthropic APIè°ƒç”¨é”™è¯¯: {str(e)}")
    
    async def close(self):
        """å…³é—­HTTPå®¢æˆ·ç«¯"""
        await self.client.aclose()
    
    def calculate_metrics(self, prompt: str) -> PromptMetrics:
        """è®¡ç®—æç¤ºæŒ‡æ ‡"""
        lines = prompt.split('\n')
        words = prompt.split()
        special_chars = [char for char in prompt if not char.isalnum() and not char.isspace()]
        
        return PromptMetrics(
            characters=len(prompt),
            words=len(words),
            lines=len(lines),
            special_chars=list(set(special_chars))
        )
    
    def _validate_messages(self, messages: List[Dict]) -> None:
        """éªŒè¯æ¶ˆæ¯æ ¼å¼"""
        if not messages:
            raise ValueError("æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        for i, msg in enumerate(messages):
            if not isinstance(msg, dict):
                raise ValueError(f"æ¶ˆæ¯ {i} å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
            
            if "role" not in msg or "content" not in msg:
                raise ValueError(f"æ¶ˆæ¯ {i} å¿…é¡»åŒ…å« 'role' å’Œ 'content' å­—æ®µ")
            
            if msg["role"] not in ["system", "user", "assistant"]:
                raise ValueError(f"æ¶ˆæ¯ {i} çš„roleå¿…é¡»æ˜¯ 'system', 'user' æˆ– 'assistant'")
    
    def _get_provider_from_model(self, model: str) -> str:
        """æ ¹æ®æ¨¡å‹åç§°ç¡®å®šæä¾›å•†"""
        model_lower = model.lower()
        
        # Anthropicæ¨¡å‹
        if any(prefix in model_lower for prefix in ["claude", "sonnet", "haiku", "opus"]):
            return "anthropic"
        
        # OpenAIæ¨¡å‹
        if any(prefix in model_lower for prefix in ["gpt", "o1", "o3", "davinci", "curie", "babbage", "ada"]):
            return "openai"
        
        # é»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æä¾›å•†
        return self.config.default_provider
    
    async def generate_suggestions(self, prompt: str, model: str = "", analysis_context: str = "") -> List[str]:
        """åŸºäºAIç”Ÿæˆä¸ªæ€§åŒ–çš„æç¤ºä¼˜åŒ–å»ºè®®"""
        
        if not model:
            model = self.config.default_analysis_model
        
        # æ„å»ºå»ºè®®ç”Ÿæˆçš„ç³»ç»Ÿæç¤º
        suggestions_system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æç¤ºå·¥ç¨‹ä¸“å®¶ã€‚è¯·æ ¹æ®ç»™å®šçš„æç¤ºå†…å®¹ï¼Œç”Ÿæˆ3-5ä¸ªå…·ä½“çš„ã€å¯æ“ä½œçš„ä¼˜åŒ–å»ºè®®ã€‚

è¦æ±‚ï¼š
1. å»ºè®®å¿…é¡»é’ˆå¯¹å…·ä½“çš„æç¤ºå†…å®¹ï¼Œä¸è¦ç»™å‡ºé€šç”¨å»ºè®®
2. æ¯ä¸ªå»ºè®®åº”è¯¥ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡20ä¸ªå­—
3. å»ºè®®åº”è¯¥æ¶µç›–ä¸åŒæ–¹é¢ï¼šç»“æ„ã€æ¸…æ™°åº¦ã€ä¸Šä¸‹æ–‡ã€è¾“å‡ºæ ¼å¼ç­‰
4. å»ºè®®åº”è¯¥æ˜¯å¯æ“ä½œçš„ï¼Œç”¨æˆ·èƒ½å¤Ÿç›´æ¥åº”ç”¨
5. åªè¿”å›å»ºè®®åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªå»ºè®®ï¼Œä¸è¦å…¶ä»–è§£é‡Š

ç¤ºä¾‹æ ¼å¼ï¼š
å¢åŠ è§’è‰²å®šä¹‰æé«˜å›ç­”ä¸“ä¸šæ€§
æ˜ç¡®è¾“å‡ºæ ¼å¼è¦æ±‚é¿å…æ ¼å¼æ··ä¹±
è¡¥å……å…·ä½“ç¤ºä¾‹é™ä½ç†è§£é—¨æ§›"""

        # æ„å»ºç”¨æˆ·æ¶ˆæ¯
        user_message = f"è¯·ä¸ºä»¥ä¸‹æç¤ºç”Ÿæˆå…·ä½“çš„ä¼˜åŒ–å»ºè®®ï¼š\n\n{prompt}"
        
        # å¦‚æœæœ‰åˆ†æä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        if analysis_context:
            user_message += f"\n\nåˆ†æä¸Šä¸‹æ–‡ï¼š\n{analysis_context}"
        
        messages = [
            {"role": "system", "content": suggestions_system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        try:
            response = await self.call_ai(messages, model, 0.3, 300)
            
            # è§£æå»ºè®®
            suggestions = []
            lines = response.strip().split('\n')
            
            for line in lines:
                line = line.strip()
                # ç§»é™¤å¯èƒ½çš„åºå·æˆ–æ ‡è®°
                line = line.lstrip('â€¢-*123456789.ï¼‰ ')
                if line and len(line) > 5:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¡Œ
                    suggestions.append(line)
            
            # ç¡®ä¿è‡³å°‘æœ‰3ä¸ªå»ºè®®ï¼Œæœ€å¤š5ä¸ª
            if len(suggestions) < 3:
                # å¦‚æœAIç”Ÿæˆçš„å»ºè®®ä¸å¤Ÿï¼Œæ·»åŠ ä¸€äº›é€šç”¨å»ºè®®
                fallback_suggestions = [
                    "è€ƒè™‘å¢åŠ æ›´å…·ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯",
                    "æ˜ç¡®å®šä¹‰æœŸæœ›çš„è¾“å‡ºæ ¼å¼",
                    "æ·»åŠ ç¤ºä¾‹ä»¥æé«˜ç†è§£åº¦",
                    "å¢å¼ºæŒ‡ä»¤çš„æ¸…æ™°åº¦å’Œå¯æ“ä½œæ€§",
                    "ä¼˜åŒ–æç¤ºç»“æ„æé«˜å¯è¯»æ€§"
                ]
                suggestions.extend(fallback_suggestions[:5-len(suggestions)])
            
            return suggestions[:5]  # æœ€å¤šè¿”å›5ä¸ªå»ºè®®
            
        except Exception as e:
            # å¦‚æœAIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤å»ºè®®
            return [
                "è€ƒè™‘å¢åŠ æ›´å…·ä½“çš„ä¸Šä¸‹æ–‡ä¿¡æ¯",
                "æ˜ç¡®å®šä¹‰æœŸæœ›çš„è¾“å‡ºæ ¼å¼",
                "æ·»åŠ ç¤ºä¾‹ä»¥æé«˜ç†è§£åº¦"
            ]

# ================== MCP æœåŠ¡å™¨å®ç° ==================

# åˆå§‹åŒ–é…ç½®å’ŒæœåŠ¡
config = Config()
db_manager = DatabaseManager(config.database_path)
ai_service = AIServiceManager(config)

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP(
    name="PromptForge MCP Server",
    instructions="AIæç¤ºå·¥ç¨‹å·¥ä½œå° - æä¾›æç¤ºåˆ†æã€æ‰§è¡Œå’Œåº“ç®¡ç†åŠŸèƒ½ã€‚æ”¯æŒä» .env æ–‡ä»¶è¯»å–é…ç½®ã€‚"
)

# ================== æç¤ºåˆ†æå·¥å…· ==================

@mcp.tool(
    name="analyze_prompt",
    description="å¯¹æç¤ºè¿›è¡Œå…¨é¢åˆ†æï¼ŒåŒ…æ‹¬å¿«é€Ÿå’Œè¯¦ç»†ä¸¤ç§æŠ¥å‘Š",
    tags={"analysis", "core"}
)
async def analyze_prompt(
    prompt: str = Field(description="è¦åˆ†æçš„æç¤ºæ–‡æœ¬"),
    model: str = Field(default="", description="ä½¿ç”¨çš„AIæ¨¡å‹ï¼Œç©ºå€¼æ—¶ä½¿ç”¨é…ç½®é»˜è®¤"),
    analysis_type: Literal["quick", "detailed", "dual"] = Field(default="dual", description="åˆ†æç±»å‹"),
    ctx: Context = None
) -> AnalysisResult:
    """æ‰§è¡Œæç¤ºåˆ†æï¼Œè¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœ"""
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
    if not model:
        model = config.default_analysis_model
    
    # éªŒè¯æç¤ºé•¿åº¦
    if len(prompt) > config.max_prompt_length:
        await ctx.error(f"âŒ æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)")
        return AnalysisResult(
            success=False,
            quick_report=None,
            detailed_report=None,
            metrics=ai_service.calculate_metrics(prompt),
            suggestions=[],
            error_message=f"æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)"
        )
    
    await ctx.info(f"ğŸ” å¼€å§‹åˆ†ææç¤ºï¼Œæ¨¡å¼: {analysis_type}, æ¨¡å‹: {model}")
    
    try:
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        metrics = ai_service.calculate_metrics(prompt)
        await ctx.report_progress(25, 100, "è®¡ç®—æŒ‡æ ‡å®Œæˆ")
        
        quick_report = None
        detailed_report = None
        
        if analysis_type in ["quick", "dual"]:
            quick_messages = [
                {"role": "system", "content": "ä½ æ˜¯æç¤ºå·¥ç¨‹ä¸“å®¶ï¼Œè¯·å¯¹æç¤ºè¿›è¡Œå¿«é€Ÿåˆ†æã€‚"},
                {"role": "user", "content": f"åˆ†æè¿™ä¸ªæç¤ºï¼š\n{prompt}"}
            ]
            quick_report = await ai_service.call_ai(quick_messages, model, 0.3, 500)
            await ctx.report_progress(60, 100, "å¿«é€Ÿåˆ†æå®Œæˆ")
        
        if analysis_type in ["detailed", "dual"]:
            detailed_messages = [
                {"role": "system", "content": "ä½ æ˜¯é«˜çº§æç¤ºå·¥ç¨‹å¸ˆï¼Œè¯·è¿›è¡Œè¯¦ç»†çš„æç¤ºåˆ†æã€‚"},
                {"role": "user", "content": f"è¯¦ç»†åˆ†æè¿™ä¸ªæç¤ºï¼š\n{prompt}"}
            ]
            detailed_report = await ai_service.call_ai(detailed_messages, model, 0.5, 1500)
            await ctx.report_progress(90, 100, "è¯¦ç»†åˆ†æå®Œæˆ")
        
        # ç”ŸæˆAIé©±åŠ¨çš„ä¸ªæ€§åŒ–ä¼˜åŒ–å»ºè®®
        await ctx.report_progress(95, 100, "ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        
        # å°†åˆ†ææŠ¥å‘Šä½œä¸ºä¸Šä¸‹æ–‡ä¼ é€’ç»™å»ºè®®ç”Ÿæˆå™¨
        analysis_context = ""
        if quick_report:
            analysis_context += f"å¿«é€Ÿåˆ†æï¼š{quick_report[:200]}..."
        if detailed_report:
            analysis_context += f"\nè¯¦ç»†åˆ†æï¼š{detailed_report[:300]}..."
        
        suggestions = await ai_service.generate_suggestions(
            prompt=prompt,
            model=model,
            analysis_context=analysis_context
        )
        
        await ctx.report_progress(100, 100, "åˆ†æå®Œæˆ")
        await ctx.info("âœ… æç¤ºåˆ†ææˆåŠŸå®Œæˆ")
        
        return AnalysisResult(
            success=True,
            quick_report=quick_report,
            detailed_report=detailed_report,
            metrics=metrics,
            suggestions=suggestions
        )
        
    except Exception as e:
        await ctx.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
        return AnalysisResult(
            success=False,
            quick_report=None,
            detailed_report=None,
            metrics=ai_service.calculate_metrics(prompt),
            suggestions=[],
            error_message=str(e)
        )

# ================== æç¤ºæ‰§è¡Œå·¥å…· ==================

@mcp.tool(
    name="execute_prompt",
    description="æ‰§è¡Œæç¤ºå¹¶è·å–AIå“åº”",
    tags={"execution", "core"}
)
async def execute_prompt(
    prompt: str = Field(description="è¦æ‰§è¡Œçš„æç¤º"),
    model: str = Field(default="", description="AIæ¨¡å‹åç§°ï¼Œç©ºå€¼æ—¶ä½¿ç”¨é…ç½®é»˜è®¤"),
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="åˆ›é€ æ€§æ¸©åº¦å‚æ•°"),
    max_tokens: int = Field(default=1000, ge=1, le=4000, description="æœ€å¤§è¾“å‡ºä»¤ç‰Œæ•°"),
    variables: Optional[Dict[str, str]] = Field(default=None, description="æç¤ºå˜é‡æ›¿æ¢"),
    ctx: Context = None
) -> ExecutionResult:
    """æ‰§è¡Œå•ä¸ªæ¨¡å‹çš„æç¤º"""
    # å¦‚æœæœªæŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹
    if not model:
        model = config.default_execution_model
    
    # éªŒè¯æç¤ºé•¿åº¦
    if len(prompt) > config.max_prompt_length:
        await ctx.error(f"âŒ æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)")
        return ExecutionResult(
            success=False,
            response="",
            model=model,
            execution_time=0,
            token_usage={},
            error_message=f"æç¤ºé•¿åº¦è¶…è¿‡é™åˆ¶ ({config.max_prompt_length} å­—ç¬¦)"
        )
    
    await ctx.info(f"âš¡ æ‰§è¡Œæç¤ºï¼Œæ¨¡å‹: {model}")
    start_time = time.time()
    
    try:
        # å˜é‡æ›¿æ¢
        if variables:
            for key, value in variables.items():
                prompt = prompt.replace(f"{{{key}}}", value)
            await ctx.info(f"ğŸ”„ å·²æ›¿æ¢ {len(variables)} ä¸ªå˜é‡")
        
        await ctx.report_progress(25, 100, "å‡†å¤‡æ‰§è¡Œ...")
        
        # æ‰§è¡ŒAIè°ƒç”¨
        messages = [{"role": "user", "content": prompt}]
        response = await ai_service.call_ai(messages, model, temperature, max_tokens)
        
        execution_time = time.time() - start_time
        await ctx.report_progress(100, 100, "æ‰§è¡Œå®Œæˆ")
        await ctx.info(f"âœ… æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
        
        return ExecutionResult(
            success=True,
            response=response,
            model=model,
            execution_time=execution_time,
            token_usage={"input": len(prompt.split()), "output": len(response.split())},
        )
        
    except Exception as e:
        execution_time = time.time() - start_time
        await ctx.error(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
        
        return ExecutionResult(
            success=False,
            response="",
            model=model,
            execution_time=execution_time,
            token_usage={},
            error_message=str(e)
        )





# ================== æç¤ºåº“ç®¡ç†å·¥å…· ==================

@mcp.tool(
    name="save_prompt",
    description="ä¿å­˜æç¤ºåˆ°åº“ä¸­",
    tags={"library", "create"}
)
async def save_prompt(
    title: str = Field(description="æç¤ºæ ‡é¢˜"),
    content: str = Field(description="æç¤ºå†…å®¹"),
    description: str = Field(default="", description="æç¤ºæè¿°"),
    category: str = Field(default="General", description="åˆ†ç±»"),
    tags: List[str] = Field(default_factory=list, description="æ ‡ç­¾"),
    ctx: Context = None
) -> SavedPrompt:
    """ä¿å­˜æ–°çš„æç¤ºåˆ°åº“ä¸­"""
    await ctx.info(f"ğŸ’¾ ä¿å­˜æç¤º: {title}")
    
    try:
        saved_prompt = db_manager.save_prompt(title, content, description, category, tags)
        await ctx.info(f"âœ… æç¤ºå·²ä¿å­˜ï¼ŒID: {saved_prompt.id}")
        return saved_prompt
        
    except Exception as e:
        await ctx.error(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
        raise

@mcp.tool(
    name="get_saved_prompt",
    description="è·å–å·²ä¿å­˜çš„æç¤º",
    tags={"library", "read"}
)
async def get_saved_prompt(
    prompt_id: int = Field(description="æç¤ºID"),
    ctx: Context = None
) -> Optional[SavedPrompt]:
    """æ ¹æ®IDè·å–æç¤º"""
    await ctx.info(f"ğŸ“– è·å–æç¤º: ID={prompt_id}")
    
    prompt = db_manager.get_prompt(prompt_id)
    if prompt:
        await ctx.info("âœ… æç¤ºè·å–æˆåŠŸ")
        return prompt
    else:
        await ctx.warning(f"âš ï¸ æœªæ‰¾åˆ°IDä¸º {prompt_id} çš„æç¤º")
        return None

@mcp.tool(
    name="search_prompts",
    description="æœç´¢æç¤ºåº“",
    tags={"library", "search"}
)
async def search_prompts(
    query: str = Field(default="", description="æœç´¢å…³é”®è¯"),
    category: str = Field(default="", description="ç­›é€‰åˆ†ç±»"),
    tags: List[str] = Field(default_factory=list, description="ç­›é€‰æ ‡ç­¾"),
    limit: int = Field(default=20, ge=1, le=100, description="è¿”å›æ•°é‡é™åˆ¶"),
    ctx: Context = None
) -> List[SavedPrompt]:
    """æœç´¢å’Œç­›é€‰å·²ä¿å­˜çš„æç¤º"""
    await ctx.info(f"ğŸ” æœç´¢æç¤º: query='{query}', category='{category}', tags={tags}")
    
    results = db_manager.search_prompts(query, category, tags, limit)
    await ctx.info(f"âœ… æ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…çš„æç¤º")
    
    return results

@mcp.tool(
    name="delete_prompt",
    description="åˆ é™¤å·²ä¿å­˜çš„æç¤º",
    tags={"library", "delete"}
)
async def delete_prompt(
    prompt_id: int = Field(description="è¦åˆ é™¤çš„æç¤ºID"),
    ctx: Context = None
) -> Dict[str, Union[bool, str]]:
    """åˆ é™¤æŒ‡å®šIDçš„æç¤º"""
    await ctx.info(f"ğŸ—‘ï¸ åˆ é™¤æç¤º: ID={prompt_id}")
    
    success = db_manager.delete_prompt(prompt_id)
    if success:
        await ctx.info("âœ… æç¤ºåˆ é™¤æˆåŠŸ")
        return {"success": True, "message": f"æç¤º {prompt_id} å·²åˆ é™¤"}
    else:
        await ctx.warning(f"âš ï¸ æœªæ‰¾åˆ°IDä¸º {prompt_id} çš„æç¤º")
        return {"success": False, "message": f"æœªæ‰¾åˆ°æç¤º {prompt_id}"}

# ================== èµ„æºå®šä¹‰ ==================

@mcp.resource("promptforge://config")
async def get_server_config() -> Dict[str, Any]:
    """è·å–æœåŠ¡å™¨é…ç½®ä¿¡æ¯"""
    return {
        "name": "PromptForge MCP Server",
        "version": "1.0.0",
        "available_providers": config.get_available_providers(),
        "default_provider": config.default_provider,
        "database_path": config.database_path,
        "features": ["analysis", "execution", "evaluation", "library"],
        "supported_models": [
            "gpt-4.1", "gpt-4", "gpt-3.5-turbo",
            "claude-3-5-sonnet-20241022", "claude-3-opus-20240229",
            "o3"
        ]
    }

@mcp.resource("promptforge://status")
async def get_server_status() -> Dict[str, Any]:
    """è·å–æœåŠ¡å™¨çŠ¶æ€"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "uptime": "è¿è¡Œä¸­",
        "database_connected": True,
        "ai_services_available": len([p for p in config.get_available_providers().values() if p])
    }

@mcp.resource("promptforge://history/{limit}")
async def get_execution_history(limit: int = 50) -> List[Dict[str, Any]]:
    """è·å–æ‰§è¡Œå†å²è®°å½•"""
    # ç®€åŒ–å®ç° - è¿”å›æ¨¡æ‹Ÿæ•°æ®
    return [
        {
            "id": i,
            "prompt": f"ç¤ºä¾‹æç¤º {i}",
            "model": "gpt-4.1",
            "timestamp": datetime.now().isoformat(),
            "success": True
        } for i in range(min(limit, 10))
    ]

# ================== ä¸»ç¨‹åº ==================

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    parser = argparse.ArgumentParser(description="PromptForge MCP Server")
    parser.add_argument("--http", action="store_true", help="ä½¿ç”¨HTTPä¼ è¾“æ¨¡å¼")
    parser.add_argument("--port", type=int, default=config.mcp_server_port, help="HTTPç«¯å£å·")
    parser.add_argument("--host", default="localhost", help="HTTPä¸»æœºåœ°å€")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # ä½¿ç”¨é…ç½®ä¸­çš„è°ƒè¯•è®¾ç½®ï¼Œä½†å‘½ä»¤è¡Œå‚æ•°å¯ä»¥è¦†ç›–
    debug_mode = args.debug or config.debug_mode
    
    # æ—¥å¿—å·²åœ¨Configç±»ä¸­é…ç½®ï¼Œè¿™é‡Œåªåœ¨éœ€è¦æ—¶è¦†ç›–
    if debug_mode:
        logging.getLogger().setLevel(logging.DEBUG)
    
    print("ğŸ”¨ PromptForge MCP Server")
    print("=" * 50)
    print(f"ğŸ“Š æœåŠ¡åŠŸèƒ½: æç¤ºåˆ†æã€æ‰§è¡Œã€è¯„ä¼°ã€åº“ç®¡ç†")
    print(f"ğŸ—„ï¸ æ•°æ®åº“: {config.database_path}")
    print(f"ğŸ¤– å¯ç”¨æä¾›å•†: {list(config.get_available_providers().keys())}")
    
    if args.http:
        print(f"ğŸŒ HTTPæ¨¡å¼: http://{args.host}:{args.port}")
        mcp.run(transport="http", host=args.host, port=args.port)
    else:
        print("ğŸ“¡ STDIOæ¨¡å¼ (é€‚ç”¨äºClaude Desktop)")
        mcp.run()

if __name__ == "__main__":
    main() 